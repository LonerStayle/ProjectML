{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89492c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_src_folder():\n",
    "    current = Path(os.getcwd()).resolve()\n",
    "    for p in [current] + list(current.parents):\n",
    "        src = p / \"src\"\n",
    "        if src.exists():\n",
    "            return src\n",
    "    raise RuntimeError(\"src í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "src_path = find_src_folder()\n",
    "sys.path.append(str(src_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f19e23",
   "metadata": {},
   "source": [
    "## BGM3_TEST\n",
    "- ë ˆì´í„´ì‹œ 0.1~0.2ì´ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ab4895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from core.common import get_src_path\n",
    "pdf_path = get_src_path() / \"tests\"/\"fairy\"/\"2501.03468v1.pdf\"\n",
    "\n",
    "def load_pdf(path):\n",
    "    reader = PdfReader(path)\n",
    "    texts = []\n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            texts.append(text)\n",
    "    return \"\\n\".join(texts)\n",
    "\n",
    "raw_text = load_pdf(pdf_path)\n",
    "print(\"ğŸ“„ PDF í…ìŠ¤íŠ¸ ê¸¸ì´:\", len(raw_text))\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,       # bge-m3ëŠ” ê¸¸ì´ ê°•í•¨. 800~1500 ì¶”ì²œ\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(raw_text)\n",
    "\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=True)\n",
    "\n",
    "import chromadb\n",
    "client = chromadb.PersistentClient(path=\"./chroma_bge_m3_demo\")\n",
    "collection = client.get_or_create_collection(name=\"bge-m3-test\")\n",
    "\n",
    "ids = [f\"chunk-{i}\" for i in range(len(chunks))]\n",
    "metas = [{\"source\": \"2501.03468v1\", \"chunk_id\": i} for i in range(len(chunks))]\n",
    "emb = model.encode(\n",
    "    chunks,\n",
    "    batch_size=8,\n",
    "    max_length=8192,\n",
    "    return_dense=True,\n",
    "    return_sparse=False,\n",
    "    return_colbert_vecs=False,\n",
    ")\n",
    "\n",
    "dense_vecs = emb[\"dense_vecs\"].tolist()\n",
    "\n",
    "collection.add(\n",
    "    ids=ids,\n",
    "    documents=chunks,\n",
    "    embeddings=dense_vecs,\n",
    "    metadatas=metas,\n",
    ")\n",
    "print(f\"ğŸ”¹ ì²­í¬ ê°œìˆ˜: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cac90b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['chunk-29']], 'embeddings': None, 'documents': [['Subset R@5\\nBy Turn Turn 1 (102) 0.89\\n> Turn 1 (675) 0.47\\nBy Standalone Standalone (555) 0.48\\n(> Turn 1) Non-Standalone (120) 0.42\\nBy Domain CLAPNQ (208) 0.56\\nFiQA (180) 0.50\\nGovt (201) 0.56\\nCloud (188) 0.47\\nTable 4: Elser retrieval results with query rewrite on\\nsubsets of the data to highlight multi-turn properties.\\nNumbers in parentheses denote size of each subset.\\n6 Generation\\nWe next present the generator experiments. We\\nstart with the experimental setup, followed by the\\nresults, using automated metrics including LLM\\njudges. Section 7 will complement this with a hu-\\nman evaluation on a subset of MTRAG. Given a\\ntask, we send to the model the following informa-\\ntion: the question, preceding turns, N passages, and\\ninstructions. We choose N=5 passages because it\\nachieves considerable improvement compared to\\ntop 3, while remaining a manageable amount of\\npassages (Section 5). For more generation format\\ndetails, see Appendix D.2.\\n6.1 Retrieval Settings']], 'uris': None, 'included': ['metadatas', 'documents', 'distances'], 'data': None, 'metadatas': [[{'source': '2501.03468v1', 'chunk_id': 29}]], 'distances': [[0.8965941667556763]]}\n"
     ]
    }
   ],
   "source": [
    "query_text = \"ì´ ë…¼ë¬¸ì—ì„œ multi-turn agent êµ¬ì¡°ëŠ” ì–´ë–»ê²Œ ì„¤ëª…ë˜ê³  ìˆëŠ”ê°€?\"\n",
    "\n",
    "query_emb = model.encode(\n",
    "    [query_text],\n",
    "    return_dense=True,\n",
    "    return_sparse=False,\n",
    "    return_colbert_vecs=False,\n",
    ")[\"dense_vecs\"].tolist()\n",
    "\n",
    "# 8) Chromaì—ì„œ ë²¡í„° ê²€ìƒ‰\n",
    "results = collection.query(\n",
    "    query_embeddings=query_emb,\n",
    "    n_results=1,\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b829a1",
   "metadata": {},
   "source": [
    "# OPEN AI TEST\n",
    "- ë ˆì´í„´ì‹œ 0.3ì´ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94fd6d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenAI Embedding ê¸°ë°˜ Chroma ì €ì¥ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import chromadb\n",
    "\n",
    "openai_embedder = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\"   \n",
    ")\n",
    "\n",
    "client_openai = chromadb.PersistentClient(path=\"./chroma_openai_demo\")\n",
    "\n",
    "\n",
    "# collection_openai = client_openai.get_or_create_collection(\n",
    "#     name=\"openai-embed-test\"\n",
    "# )\n",
    "collection_openai = client_openai.get_or_create_collection(\n",
    "    name=\"openai-embed-test-large\",   # ë³€ê²½!\n",
    ")\n",
    "openai_vectors = openai_embedder.embed_documents(chunks)\n",
    "ids_openai = [f\"openai-chunk-{i}\" for i in range(len(chunks))]\n",
    "metas_openai = [{\"source\": \"2501.03468v1\", \"chunk_id\": i} for i in range(len(chunks))]\n",
    "\n",
    "collection_openai.add(\n",
    "    ids=ids_openai,\n",
    "    documents=chunks,\n",
    "    embeddings=openai_vectors,\n",
    "    metadatas=metas_openai,\n",
    ")\n",
    "\n",
    "print(\"âœ… OpenAI Embedding ê¸°ë°˜ Chroma ì €ì¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80432c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ğŸ” OpenAI Embedding ê²€ìƒ‰ ê²°ê³¼ ===\n",
      "\n",
      "--------------------------------\n",
      "ğŸ“Œ Chunk: 26\n",
      "ğŸ“„ ë‚´ìš©: We experimented with several strategies to query\n",
      "the retriever for relevant passages, including\n",
      "sending the full conversation up to the current\n",
      "user turn, all the user turns without the responses,\n",
      "sub ...\n",
      "ğŸ“‰ ê±°ë¦¬: 1.0796127319335938\n",
      "\n",
      "--------------------------------\n",
      "ğŸ“Œ Chunk: 21\n",
      "ğŸ“„ ë‚´ìš©: generator outputs as needed. Using the application,\n",
      "annotators created multi-turn conversations by per-\n",
      "forming the following actions at every turn: write\n",
      "a question, adjust the set of retrieved passa ...\n",
      "ğŸ“‰ ê±°ë¦¬: 1.1401104927062988\n"
     ]
    }
   ],
   "source": [
    "query_text = \"ì´ ë…¼ë¬¸ì—ì„œ multi-turn agent êµ¬ì¡°ëŠ” ì–´ë–»ê²Œ ì„¤ëª…ë˜ëŠ”ê°€?\"\n",
    "\n",
    "query_emb_openai = openai_embedder.embed_query(query_text)\n",
    "\n",
    "results_openai = collection_openai.query(\n",
    "    query_embeddings=[query_emb_openai],\n",
    "    n_results=2,\n",
    ")\n",
    "\n",
    "print(\"\\n=== ğŸ” OpenAI Embedding ê²€ìƒ‰ ê²°ê³¼ ===\")\n",
    "for doc, meta, dist in zip(\n",
    "    results_openai[\"documents\"][0],\n",
    "    results_openai[\"metadatas\"][0],\n",
    "    results_openai[\"distances\"][0],\n",
    "):\n",
    "    print(\"\\n--------------------------------\")\n",
    "    print(\"ğŸ“Œ Chunk:\", meta[\"chunk_id\"])\n",
    "    print(\"ğŸ“„ ë‚´ìš©:\", doc[:200], \"...\")\n",
    "    print(\"ğŸ“‰ ê±°ë¦¬:\", dist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProjectML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
